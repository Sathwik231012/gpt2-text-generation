# GPT-2 Text Generation

A demo Python project that illustrates creating text generation based on the GPT-2 language model of Hugging Face Transformers.

## Project Overview

Through this project, the user can feed any text prompt and receive a human-like text completion based on using the pretrained GPT-2 model. GPT-2 is an advanced deep learning model made by OpenAI which has been trained on natural language generation.

## Features

- Create logical and imaginative text in accordance with user requests.
- Adjustable generation parameters such as maximum length and temperature (creativity).
Convenient command line choice of interacting with the model in real-time.
Aplicable to creative writing, chatbots, tale producing and instruction transformer-based NLP systems.

## Installation

Ensure that you install Python 3.7 or higher. A virtual environment is suggested to be used.
