# GPT-2 Text Generation

A demo Python project that illustrates creating text generation based on the GPT-2 language model of Hugging Face Transformers.

## Project Overview

Through this project, the user can feed any text prompt and receive a human-like text completion based on using the pretrained GPT-2 model. GPT-2 is an advanced deep learning model made by OpenAI which has been trained on natural language generation.

## Features

- Create logical and imaginative text in accordance with user requests.
- Adjustable generation parameters such as maximum length and temperature (creativity).
Convenient command line choice of interacting with the model in real-time.
Aplicable to creative writing, chatbots, tale producing and instruction transformer-based NLP systems.

## Installation

Ensure that you install Python 3.7 or higher. A virtual environment is suggested to be used.

Install dependencies using pip:

pip install transformers torch

## Usage

python gpt2_text_generation.py

Enter a prompt when asked

Type exit to quit

## Files

gpt2_text_generation.py — Main script

README.md — This file

requirements.txt - Required libraries

## Author
Sathwik — (https://github.com/Sathwik231012)
